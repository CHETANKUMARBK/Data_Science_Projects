{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the mian data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = data[['Date', 'Species', 'Trap', 'Latitude', 'Longitude', 'NumMosquitos', 'WnvPresent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the total number of mos of same species for one trap each day\n",
    "mos = data.groupby(['Date', 'Species', 'Trap', 'Latitude', 'Longitude']).NumMosquitos.sum()\n",
    "wnv = data.groupby(['Date', 'Species', 'Trap', 'Latitude', 'Longitude']).WnvPresent.sum()\n",
    "\n",
    "# put number of mos and virus present back together\n",
    "df = pd.concat([mos, wnv], axis=1)\n",
    "df.reset_index(level=['Date', 'Species', 'Trap', 'Latitude', 'Longitude'], inplace=True)\n",
    "df.WnvPresent = df.WnvPresent.apply(lambda x: 1 if x>=1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather.drop('CodeSum', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = weather.replace('M', -1)\n",
    "weather = weather.replace('-', -1)\n",
    "weather = weather.replace('T', -1)\n",
    "weather = weather.replace(' T', -1)\n",
    "weather = weather.replace('  T', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather.Tavg = pd.to_numeric(weather.Tavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split weather data from different stations\n",
    "\n",
    "weather1 = weather[weather['Station']==1]\n",
    "weather2 = weather[weather['Station']==2]\n",
    "weather1 = weather1.drop('Station', axis=1)\n",
    "weather2 = weather2.drop('Station', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename columns so that after feature selection I could identify where the feature is from\n",
    "weather1.columns = weather1.columns + str(1)\n",
    "weather1.rename(columns={'Date1': 'Date'}, inplace=True)\n",
    "weather2.columns = weather2.columns +str(2)\n",
    "weather2.rename(columns={'Date2': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge weather data to put weather data for each day in one single row\n",
    "weather = weather1.merge(weather2, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.Date = pd.to_datetime(weather.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge weather data to the main dataframe\n",
    "df = pd.merge(df, weather, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create time variables to use to merge spray data\n",
    "\n",
    "df['Year'] = df.Date.dt.year\n",
    "df['Month'] = df.Date.dt.month\n",
    "df['Week'] = df.Date.dt.week\n",
    "df['Day'] = df.Date.dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize spray data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we think one spray will have a lasting effect for 3 weeks in a certain area, \n",
    "# so the following code is essentially trying to merge spray to the main dataset on spray date and location,\n",
    "# and assaign spray=1 to the records that within 3 weeks after spray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we think spray will have a wild range of effect, so we round the coordinates to 2 decimal place and merge on that\n",
    "\n",
    "spray = pd.read_csv(\"spray.csv\")\n",
    "df['merge_latitude'] = df.Latitude.apply(lambda x: round(x, 2))\n",
    "df['merge_longitude'] = df.Longitude.apply(lambda x: round(x, 2))\n",
    "spray['merge_latitude'] = spray.Latitude.apply(lambda x: round(x, 2))\n",
    "spray['merge_longitude'] = spray.Longitude.apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.Date = pd.to_datetime(spray.Date)\n",
    "spray['Year'] = spray.Date.dt.year\n",
    "spray['Week'] = spray.Date.dt.week\n",
    "spray.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spray = spray.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following lines of code are trying to create 3 dataframe from spray to represent spray effect for three weeks\n",
    "\n",
    "spray['Week2'] = spray.Week + 1\n",
    "spray['Week3'] = spray.Week + 2\n",
    "spray['Spray1'] = 1\n",
    "spray['Spray2'] = 1\n",
    "spray['Spray3'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray_week1 = spray[['merge_latitude', 'merge_longitude', 'Year', 'Week', 'Spray1']]\n",
    "spray_week2 = spray[['merge_latitude', 'merge_longitude', 'Year', 'Week2', 'Spray2']]\n",
    "spray_week2.rename(columns={'Week2':'Week'}, inplace=True)\n",
    "spray_week3 = spray[['merge_latitude', 'merge_longitude', 'Year', 'Week3', 'Spray3']]\n",
    "spray_week3.rename(columns={'Week3':'Week'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finally, merge spray data to the main dataframe\n",
    "\n",
    "df = pd.merge(df, spray_week1, how = 'left', on=['Year', 'Week', 'merge_latitude', 'merge_longitude'])\n",
    "df = pd.merge(df, spray_week2, how = 'left', on=['Year', 'Week', 'merge_latitude', 'merge_longitude'])\n",
    "df = pd.merge(df, spray_week3, how = 'left', on=['Year', 'Week', 'merge_latitude', 'merge_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get rid of NaNs. Replace them with 0s.\n",
    "df.Spray1 = df.Spray1.apply(lambda x: 1 if x == 1 else 0)\n",
    "df.Spray2 = df.Spray2.apply(lambda x: 1 if x == 1 else 0)\n",
    "df.Spray3 = df.Spray3.apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine spray information to one single column, and drop columns used to merge\n",
    "df['Sprayed'] = df.Spray1 + df.Spray2 + df.Spray3\n",
    "df.drop(['merge_latitude', 'merge_longitude', 'Spray1', 'Spray2', 'Spray3'], axis=1, inplace=True)\n",
    "df.drop(['Date', 'Month', 'Day'], axis=1, inplace=True)\n",
    "df['wnv'] = df.WnvPresent\n",
    "df.drop('WnvPresent', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('~/desktop/wnv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/desktop/wnv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label encode all the categorical features\n",
    "\n",
    "le = LabelEncoder()\n",
    "cols = [i for i in df.select_dtypes(include=['object']).columns]\n",
    "df[cols] = df[cols].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.wnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NumMosquitos is not available in the test dataset. The number of Traps is much less than that in the test dataset.\n",
    "# Sunset time is perfectly correlated with Sunrise time, so we do not expect including it would bring extra benefits\n",
    "X = df[['Longitude', 'Latitude', 'Species', 'Sunrise1', 'Week']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=df.wnv, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use grid search to optimize the auc\n",
    "\n",
    "rf_params = {'n_estimators': [5, 10, 15, 20, 30], 'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "            'max_depth': [3, 5, 10, None]}\n",
    "rfgs = GridSearchCV(rf, rf_params, scoring='roc_auc')\n",
    "rfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfgs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rfgs.best_params_\n",
    "print rfgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the best model from grid search to calculate the cross validation score\n",
    "# set class_weights='balanced' to capture more true positive prediction\n",
    "rf = RandomForestClassifier(max_features='log2', n_estimators=15, criterion='gini', max_depth=5, class_weight='balanced')\n",
    "print cross_val_score(rf, X, y, cv=5, n_jobs=-1).mean()\n",
    "rf_pred = cross_val_predict(rf, X, y, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conmat = np.array(confusion_matrix(y, rf_pred, labels=[1,0]))\n",
    "confusion = pd.DataFrame(conmat, index=['wnv', 'no wnv'], columns=['pred wnv', 'pred no wnv'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot roc curve and calculate auc\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf_prob = rf.predict_proba(X_test)[:,1]\n",
    "rffpr = dict()\n",
    "rftpr = dict()\n",
    "rfroc_auc=dict()\n",
    "rffpr[1], rftpr[1], _ = roc_curve(y_test, rf_prob)\n",
    "rfroc_auc[1] = auc(rffpr[1], rftpr[1])\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(rffpr[1], rftpr[1], label='ROC curve (area = %0.2f)' % rfroc_auc[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for WNV (rf)', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standardize numeric features\n",
    "\n",
    "numerical_features = ['Longitude', 'Latitude', 'Sunrise1', 'Week']\n",
    "for i in numerical_features:\n",
    "    df[i] = (df[i]-df[i].mean())/df[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theoratically, we should have created dummies for categorical features, but doing that brought our kaggle score down \n",
    "\n",
    "X = df[['Longitude', 'Latitude', 'Species', 'Sunrise1', 'Week']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=df.wnv, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid search\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf_params = {'C': 10.**np.arange(-2,3), 'gamma': 10.**np.arange(-5,2)}\n",
    "clfgs = GridSearchCV(clf, clf_params, scoring='roc_auc')\n",
    "clfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfgs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clfgs.best_params_\n",
    "print clfgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "cv = StratifiedKFold(df.wnv, n_folds=5, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=100, gamma=1, probability=True, class_weight='balanced')\n",
    "clfscore = cross_val_score(clf, X, y, cv=cv, n_jobs=-1).mean()\n",
    "clf_pred = cross_val_predict(clf, X, y, cv=cv, n_jobs=-1)\n",
    "print clfscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm_clf = np.array(confusion_matrix(y, clf_pred, labels=[1,0]))\n",
    "confusion_clf = pd.DataFrame(cm_clf, index=['wnv', 'no wnv'], columns=['pred wnv', 'pred no wnv'])\n",
    "confusion_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf_prob = clf.predict_proba(X_test)[:,1]\n",
    "clffpr = dict()\n",
    "clftpr = dict()\n",
    "clfroc_auc=dict()\n",
    "clffpr[1], clftpr[1], _ = roc_curve(y_test, clf_prob)\n",
    "clfroc_auc[1] = auc(clffpr[1], clftpr[1])\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(clffpr[1], clftpr[1], label='ROC curve (area = %0.2f)' % clfroc_auc[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for WNV (svm)', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params={'n_neighbors': range(3, 80), 'weights': ['uniform', 'distance']}\n",
    "knngs = GridSearchCV(knn, knn_params, scoring='roc_auc')\n",
    "knngs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print knngs.best_params_\n",
    "print knngs.best_score_\n",
    "print knngs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=41, weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = X.astype(float).as_matrix()\n",
    "ys = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=5, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=0.3, stratify=df.wnv, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=20, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator.fit(xs_train, ys_train)\n",
    "nn_prob = estimator.predict_proba(xs_test)[:,1]\n",
    "nnfpr = dict()\n",
    "nntpr = dict()\n",
    "nnroc_auc=dict()\n",
    "nnfpr[1], nntpr[1], _ = roc_curve(y_test, nn_prob)\n",
    "nnroc_auc[1] = auc(nnfpr[1], nntpr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnroc_auc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_prob = gnb.predict_proba(X_test)[:,1]\n",
    "gnbfpr = dict()\n",
    "gnbtpr = dict()\n",
    "gnbroc_auc=dict()\n",
    "gnbfpr[1], gnbtpr[1], _ = roc_curve(y_test, clf_prob)\n",
    "gnbroc_auc[1] = auc(gnbfpr[1], gnbtpr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnbroc_auc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(df, x_vars='Latitude', y_vars='Longitude', hue='wnv', size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the testing dataframe. we only use weather data from station 1 because that is where our features from\n",
    "\n",
    "testing = pd.read_csv('test.csv')\n",
    "weather_test = pd.read_csv('weather.csv')\n",
    "weather_test = weather_test[weather_test.Station==1]\n",
    "testing = pd.merge(left=testing, right=weather_test, on='Date', how='left')\n",
    "testing.Date = pd.to_datetime(testing.Date)\n",
    "testing['Week'] = testing.Date.dt.week\n",
    "testing.Sunrise = pd.to_numeric(testing.Sunrise)\n",
    "testing.rename(columns={'Sunrise': 'Sunrise1'}, inplace=True)\n",
    "testing.to_csv('cleanedtest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing = pd.read_csv('cleanedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manually standardize numeric features\n",
    "for i in numerical_features:\n",
    "    testing[i] = (testing[i]-testing[i].mean())/testing[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label encoder categorical features\n",
    "test_cols = [i for i in testing.select_dtypes(include=['object']).columns]\n",
    "testing[test_cols] = testing[test_cols].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_testing = testing[['Longitude', 'Latitude', 'Species', 'Sunrise1', 'Week']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_test = rf.predict_proba(X_testing)[:,1]\n",
    "testing['WnvPresent'] = rf_test\n",
    "submit_rf = testing[['Id', 'WnvPresent']]\n",
    "submit_rf.to_csv('score_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_test = clf.predict_proba(X_testing)[:,1]\n",
    "testing['WnvPresent'] = svm_test\n",
    "submit_svm = testing[['Id', 'WnvPresent']]\n",
    "submit_svm.to_csv('score_svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_test = knn.predict_proba(X_testing)[:,1]\n",
    "testing['WnvPresent'] = knn_test\n",
    "submit_knn = testing[['Id', 'WnvPresent']]\n",
    "submit_knn.to_csv('score_knn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs_testing = X_testing.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_test = estimator.predict_proba(xs_testing)[:,1]\n",
    "testing['WnvPresent'] = nn_test\n",
    "submit_nn = testing[['Id', 'WnvPresent']]\n",
    "submit_nn.to_csv('score_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_test = gnb.predict_proba(X_testing)[:,1]\n",
    "testing['WnvPresent'] = gnb_test\n",
    "submit_gnb = testing[['Id', 'WnvPresent']]\n",
    "submit_gnb.to_csv('score_gnb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
